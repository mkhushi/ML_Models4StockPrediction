{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COPY version of project code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "otETlYbGYEHj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# COPY COMP5703 - Capstone project CP-14 ---->\n",
        "\n",
        "Members: Pote Pongchaikul, Sergio Kulikovsky, Thomas Brown, Ignacio Colino, Aditya Sharma"
      ]
    },
    {
      "metadata": {
        "id": "goWX1jkHFvir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "metadata": {
        "id": "XrVmOp9Z-NxB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# PyDrive reference:\n",
        "# https://googledrive.github.io/PyDrive/docs/build/html/index.html\n",
        "# https://pythonhosted.org/PyDrive/\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gi_ECHAc-kHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "a8f25ff7-a6eb-4ef5-d195-7cf0d99c325f"
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1mWlUOJaz6IPibDYVw8Oz1Ps_04hNwJSu' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: COPY version of project code.ipynb, id: 1IwGrPx0ScQ1z0B3OOy5EBisvDTGbM-iT\n",
            "title: COMP5703 Project code 1.ipynb, id: 1hDXK8bn-zpmzGzvIXBddWhSohkY0yEN6\n",
            "title: INDEX version of project code.ipynb, id: 1H8FzkIRn8Jk7TAHcA0rs7gYog7PhhrIO\n",
            "title: Project timeline, id: 1Hg82YX1iJozlaclxmHSCxqrVplQh81HYfyL1Hw4BZU4\n",
            "title: Capstone_Create_Datasets_27Aug18.ipynb, id: 1anV2YNGlCdiL3tBRUa5G7uNkmVWPDPkW\n",
            "title: Scribble notes, id: 1AyZhQVkclpd0NqWdU02yM23gELcRDLYV8iU-1FXeni0\n",
            "title: OLD, id: 1bKKKo4QNg1omQ_ejH3_ntUu_EMwLJvv-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MJpHSBHGF1OB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ]
    },
    {
      "metadata": {
        "id": "YSzZOQhMABvo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "19b3e474-6bcb-4178-c9ea-da4fd9ad81aa"
      },
      "cell_type": "code",
      "source": [
        "## For state space modelling\n",
        "## Ref: https://pydlm.github.io/installation.html, accessed 5 September\n",
        "!pip install pydlm\n",
        "!pip install seaborn\n",
        "!pip install wbdata\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydlm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/eb/09e3cfb7e5326b4240e9c92cd1639202e1f5c7af14b87bbf65bb76c7f7e2/pydlm-0.1.1.10.tar.gz (43kB)\n",
            "\r\u001b[K    23% |███████▋                        | 10kB 21.3MB/s eta 0:00:01\r\u001b[K    47% |███████████████▏                | 20kB 1.7MB/s eta 0:00:01\r\u001b[K    71% |██████████████████████▉         | 30kB 2.4MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▍ | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pydlm) (1.14.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pydlm) (2.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pydlm) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pydlm) (2.2.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->pydlm) (2018.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pydlm) (1.11.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pydlm) (0.10.0)\n",
            "Building wheels for collected packages: pydlm\n",
            "  Running setup.py bdist_wheel for pydlm ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d3/64/7f/99427e6464ff5f8561889ab4f001422a69e1f59636790b9f91\n",
            "Successfully built pydlm\n",
            "Installing collected packages: pydlm\n",
            "Successfully installed pydlm-0.1.1.10\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Collecting wbdata\n",
            "  Downloading https://files.pythonhosted.org/packages/d8/a0/c83a8cb001885685f8344f4a6d3557ec3de73d367aea82dbb519aa6d2706/wbdata-0.2.7.tar.gz\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from wbdata) (4.3.0)\n",
            "Building wheels for collected packages: wbdata\n",
            "  Running setup.py bdist_wheel for wbdata ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/57/61/6a/3978e90cf2f9443b94ce56b4fa839850da9076e697be3a27e3\n",
            "Successfully built wbdata\n",
            "Installing collected packages: wbdata\n",
            "Successfully installed wbdata-0.2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T0yjv69B_wCz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Import packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Ref: https://pydlm.github.io/pydlm_user_guide.html#modeling, accessed 5 September\n",
        "from pydlm import dlm, dynamic\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4j-uk7dF7pE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import dataset\n"
      ]
    },
    {
      "metadata": {
        "id": "Ecet3SRi-6ij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Adjusted data 1: ASX200\n",
        "## Adjusted data 2: S&P500 set 1\n",
        "## Adjusted data 3: S&P500 set 2\n",
        "## Adjusted data 4: S&P500 set 3\n",
        "\n",
        "\n",
        "adjusted_data_ASX = pd.read_csv(\"https://raw.github.sydney.edu.au/asha7190/MI_FinancialTrading/master/Code_old/adjusted_data.csv?token=AAAH4bQYtoFgmMMCIs74ciLBtD3taWFxks5bw_MpwA%3D%3D\",\n",
        "                           index_col=0, parse_dates=True)\n",
        "\n",
        "adjusted_data_SP500_1 = pd.read_csv(\"https://raw.github.sydney.edu.au/asha7190/MI_FinancialTrading/master/Code_old/S%26P500%20Data/sp500_adjusted_data_1.csv?token=AAAH4ffHgmhs-El1ALefeRD0kBuh95fdks5bw_NWwA%3D%3D\",\n",
        "                           index_col=0, parse_dates=True)\n",
        "\n",
        "adjusted_data_SP500_2 = pd.read_csv(\"https://raw.github.sydney.edu.au/asha7190/MI_FinancialTrading/master/Code_old/S%26P500%20Data/sp500_adjusted_data_2.csv?token=AAAH4XXEjrUCX3IsffbgqrhaSaLcwcosks5bw_O4wA%3D%3D\",\n",
        "                           index_col=0, parse_dates=True)\n",
        "\n",
        "adjusted_data_SP500_3 = pd.read_csv(\"https://raw.github.sydney.edu.au/asha7190/MI_FinancialTrading/master/Code_old/S%26P500%20Data/sp500_adjusted_data_3.csv?token=AAAH4fxQy7QhSXOVnBn_KWBRs9GSj573ks5bw_PSwA%3D%3D\",\n",
        "                           index_col=0, parse_dates=True)\n",
        "\n",
        "\n",
        "\n",
        "dataset_list = [\"ASX\",\"SP500_1\",\"SP500_2\",\"SP500_3\"]\n",
        "adjusted_data_df_original = {\"ASX\":adjusted_data_ASX, \"SP500_1\":adjusted_data_SP500_1,\n",
        "                     \"SP500_2\":adjusted_data_SP500_2, \"SP500_3\":adjusted_data_SP500_3}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oNUUmSTyNP63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Rename columns and to drop some columns that appeared redundant\n",
        "\n",
        "for data in dataset_list:\n",
        "  \n",
        "  ## Rename columns:\n",
        "  ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rename.html, accessed 3 September\n",
        "  adjusted_data_df_original[data] = adjusted_data_df_original[data].rename(columns = {\"open\":\"Open\",\"high\":\"High\",\"low\":\"Low\",\"close\":\"Close\",\"volume\":\"Volume\"})\n",
        "\n",
        "  ## Drop columns: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html, accessed 16 September\n",
        "  #adjusted_data_df_original[data] = adjusted_data_df_original[data].drop(columns = [\"dividend_amount\",\t\"split_coefficient\"])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SHULYIsziYDc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b8017432-61d3-4762-c3f0-45d4758385cc"
      },
      "cell_type": "code",
      "source": [
        "adjusted_data_df_original[\"SP500_3\"].tail()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>adjusted_close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>dividend_amount</th>\n",
              "      <th>split_coefficient</th>\n",
              "      <th>symbol</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2018-09-17</th>\n",
              "      <td>89.06</td>\n",
              "      <td>89.2800</td>\n",
              "      <td>88.030</td>\n",
              "      <td>88.30</td>\n",
              "      <td>88.30</td>\n",
              "      <td>1667207</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ZTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-09-18</th>\n",
              "      <td>88.17</td>\n",
              "      <td>89.4850</td>\n",
              "      <td>88.150</td>\n",
              "      <td>89.18</td>\n",
              "      <td>89.18</td>\n",
              "      <td>1722348</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ZTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-09-19</th>\n",
              "      <td>89.14</td>\n",
              "      <td>89.6900</td>\n",
              "      <td>88.790</td>\n",
              "      <td>89.08</td>\n",
              "      <td>89.08</td>\n",
              "      <td>2273387</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ZTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-09-20</th>\n",
              "      <td>89.72</td>\n",
              "      <td>90.5000</td>\n",
              "      <td>89.315</td>\n",
              "      <td>90.06</td>\n",
              "      <td>90.06</td>\n",
              "      <td>3428300</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ZTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018-09-21</th>\n",
              "      <td>90.03</td>\n",
              "      <td>90.9341</td>\n",
              "      <td>89.790</td>\n",
              "      <td>89.83</td>\n",
              "      <td>89.83</td>\n",
              "      <td>3448374</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ZTS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             Open     High     Low  Close  adjusted_close   Volume  \\\n",
              "date                                                                 \n",
              "2018-09-17  89.06  89.2800  88.030  88.30           88.30  1667207   \n",
              "2018-09-18  88.17  89.4850  88.150  89.18           89.18  1722348   \n",
              "2018-09-19  89.14  89.6900  88.790  89.08           89.08  2273387   \n",
              "2018-09-20  89.72  90.5000  89.315  90.06           90.06  3428300   \n",
              "2018-09-21  90.03  90.9341  89.790  89.83           89.83  3448374   \n",
              "\n",
              "            dividend_amount  split_coefficient symbol  \n",
              "date                                                   \n",
              "2018-09-17              0.0                1.0    ZTS  \n",
              "2018-09-18              0.0                1.0    ZTS  \n",
              "2018-09-19              0.0                1.0    ZTS  \n",
              "2018-09-20              0.0                1.0    ZTS  \n",
              "2018-09-21              0.0                1.0    ZTS  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "h01a4w95lFXg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import and merge World bank data"
      ]
    },
    {
      "metadata": {
        "id": "MJOMVZFuJJzS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#World Bank API\n",
        "\n",
        "# Data Acquisition\n",
        "\n",
        "# For citation purposes:\n",
        "\n",
        "# Sherouse, Oliver (2014). Wbdata. Arlington, VA. Available from http://github.com/OliverSherouse/wbdata.\n",
        "\n",
        "# Documentation: https://wbdata.readthedocs.io/en/latest/index.html\n",
        "\n",
        "#Load libraries\n",
        "import wbdata\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "#import pandas_datareader.data as web\n",
        "\n",
        "from matplotlib.dates import date2num\n",
        "from matplotlib.dates import num2date\n",
        "import matplotlib.dates as mpld\n",
        "import numpy as np\n",
        "\n",
        "#List of information sources (indicators groups/categories)\n",
        "\n",
        "#wbdata.get_source()\n",
        "\n",
        "\n",
        "#Search indicator code\n",
        "\n",
        "#wbdata.get_indicator(source=6)\n",
        "\n",
        "#Search for countries or indicators\n",
        "#wbdata.search_countries(\"New\")\n",
        "#wbdata.search_indicators(\"debt\")\n",
        "#country_list=wbdata.get_country(display=False)\n",
        "#country_list[0]\n",
        "#all_countries={}\n",
        "#for country in country_list:\n",
        "#    all_countries[country['id']]=country['name']\n",
        "\n",
        "#Get Data Demo\n",
        "#countries2=[\"AUS\",\"ARG\",\"BRA\",\"USA\"]\n",
        "#indicators2 = {'NY.GDP.PCAP.CD':'GDP per Capita', 'NY.GDP.DEFL.KD.ZG':'Inflation'}\n",
        "#df = wbdata.get_dataframe(indicators2, country=countries2, convert_date=False)\n",
        "#dfu = df.unstack(level=0)\n",
        "#plt.figure(figsize=(15,5))\n",
        "#plt.subplot(121)\n",
        "#plt.plot(dfu['GDP per Capita']);plt.title('GDP per Capita');plt.xlabel('Date');\n",
        "#plt.legend(dfu['GDP per Capita']);plt.xticks(rotation='vertical', size=8)\n",
        "#plt.subplot(122)\n",
        "#plt.plot(dfu['Inflation'][-20:]);plt.title('Inflation');plt.xlabel('Date');\n",
        "#plt.legend(dfu['Inflation']);plt.xticks(rotation='vertical')\n",
        "#plt.show()\n",
        "#df.head()\n",
        "\n",
        "\n",
        "#Print list of indicator categories and count\n",
        "#indicator_details=wbdata.get_indicator(source=2, display=False)\n",
        "#categories={}\n",
        "#\n",
        "#for indicator in indicator_details:\n",
        "#    if len(indicator['topics'])>0:\n",
        "#        if indicator['topics'][0]['value'] in categories:\n",
        "#            categories[indicator['topics'][0]['value']]+=1\n",
        "#        else:\n",
        "#            categories[indicator['topics'][0]['value']]=1\n",
        "#\n",
        "#for category in categories:\n",
        "#    print(category, categories[category])\n",
        "\n",
        "\n",
        "#Input for the function\n",
        "countries=['AUS','CHN']\n",
        "indicators={'NY.GDP.PCAP.PP.CD':'GDP per capita, PPP (current international $)',\n",
        "               'NY.GDP.MKTP.CD':'GDP (current US$)',\n",
        "               'NY.GDP.DEFL.KD.ZG':'Inflation, GDP deflator (annual %)',\n",
        "               'DT.DOD.DSTC.IR.ZS':'Short-term debt (% of total reserves)',\n",
        "               'GC.DOD.TOTL.GD.ZS':'Central government debt, total (% of GDP)',\n",
        "               'FP.CPI.TOTL.ZG':'Inflation, consumer prices (annual %)',\n",
        "               'FI.RES.TOTL.CD':'Total reserves (includes gold, current US$)',\n",
        "               'NV.IND.TOTL.CD':'Industry (including construction), value added (current US$)',\n",
        "               'NV.IND.MANF.CD':'Manufacturing, value added (current US$)',\n",
        "               'NE.TRD.GNFS.ZS':'Trade (% of GDP)',\n",
        "               }\n",
        "\n",
        "\n",
        "data_date = (datetime.datetime(1999, 1, 1), datetime.datetime(2018, 9, 1))\n",
        "\n",
        "#To inspect indicator categories\n",
        "\n",
        "#for indicator in indicator_details:\n",
        "#    if len(indicator['topics'])>0:\n",
        "#        if indicator['topics'][0]['value']=='Health ':\n",
        "#            print(\"'\",indicator['id'],\"'\",':',\"'\", indicator['name'],\"'\", sep='')\n",
        "\n",
        "\n",
        "#Get the data\n",
        "def get_wbdataframe(indicators, data_date, countries):\n",
        "    df=wbdata.get_dataframe(indicators=indicators, \n",
        "                            data_date=data_date, \n",
        "                            convert_date=True, \n",
        "                            country=countries) \n",
        "    df=df.unstack(level=0)\n",
        "    df.columns=['-'.join(col).strip() for col in df.columns.values]\n",
        "    ## print(df.shape, 'Shape with null columns')\n",
        "    df=df.loc[:,(df.sum(axis=0)!=0)]\n",
        "    df=df.dropna(axis='columns', how='all')\n",
        "    ## print(df.shape, 'Shape without null columns')\n",
        "    \n",
        "    return df\n",
        "\n",
        "#df= get_wbdataframe(indicators, data_date, countries)\n",
        "\n",
        "#df.to_csv('wb_data.csv', index=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "agPMSfoLQ0lD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def merge_wbdata(stock,stock_abb):\n",
        "\n",
        "  #from get_wb_data import get_wbdataframe, indicators, countries, data_date\n",
        "  #stock=adjusted_data_df_original[data]\n",
        "\n",
        "  #Get Wb data from csv\n",
        "  ##wb_data=pd.read_csv('wb_data.csv', index_col=0, parse_dates=True).astype(np.float32)\n",
        "\n",
        "  #Get Wb data from api\n",
        "  wb_data=get_wbdataframe(indicators, data_date, countries)\n",
        "\n",
        "  #Create dataframe for a single stock\n",
        "  temp_df=stock.loc[stock.symbol==stock_abb]\n",
        "  temp_df.sort_index(inplace=True)\n",
        "\n",
        "  temp_df=pd.merge_asof(temp_df, wb_data, left_index=True, \n",
        "                    right_index=True, tolerance=pd.Timedelta(days=731))\n",
        "  return temp_df\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-o1KOsOGPeq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature engineering\n",
        "\n",
        "These are taken from project proposal, but might need further revision\n",
        "\n",
        "1. Price o Normalized to a similar scale\n",
        "2. Volume\n",
        "3. Moving average: Test different periods: 5, 10, 20, 50, 100, 150, 200, 250 day periods\n",
        "4. Momentum: Change in price over periods: 1-250 days\n",
        "5. Exponentially weighted moving average: Test different periods: 5, 10, 15, 20, 25, 30 days and different exponential weights\n",
        "6. Volatily: Test different periods: 5, 10, 20, 50, 100, 150, 200, 250 day periods\n",
        "7. Autocorrelation: Test different periods. In this specific case we shall feature autocorrelation between 10-250 days.\n",
        "8. Relative Strength Index, RSI: We shall engineer RSI data for periods from 5 to 250 days.\n",
        "9. Standard deviation of price: Test different periods: 5-50 days\n",
        "10. Volume traded moving average: Test different periods: 5, 10, 20, 50, 100, 150, 200, 250 day periods\n",
        "11. On balance volume (sum volume if price increases, deduct if it decreases): Test different periods\n",
        "12. Money flow index: Similar to on-balance volume, measures volume and price. We intend to test for different periods\n",
        "13. Apart from doing extensive feature engineering we also intend to do Spectral Analysis using Fourier Transformation of stock-price time series."
      ]
    },
    {
      "metadata": {
        "id": "YCQp5wCKSsKU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "metadata": {
        "id": "i9oS-CPXfGgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "## Feature engineering\n",
        "### Acknowledgment: Dr. Matloob Khushi \n",
        "#######################################\n",
        "\n",
        "## define functions to calculate technical indicators - SET 1\n",
        "\n",
        "def SMA(data,period):\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = data\n",
        "    return np.array(temp_df.Close.rolling(period).mean())\n",
        "\n",
        "def WMA(data,period):\n",
        "    temp = []\n",
        "    for i in np.arange(0,len(data)):\n",
        "        if i < period-1:\n",
        "            temp.append(np.nan)\n",
        "        else:\n",
        "            num, denom  = 0, 0\n",
        "            for j in np.arange(0,period):\n",
        "                num += (period - j) * data[i-j]\n",
        "                denom += (period - j)\n",
        "            temp.append(num / denom)\n",
        "    return np.array(temp)\n",
        "\n",
        "def Momentum(data,period):\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = data\n",
        "    return np.array(temp_df.Close - temp_df.Close.shift(period))\n",
        "\n",
        "def Stochastic_K(close,high,low,period=14):\n",
        "#     %K = 100(C - L14)/(H14 - L14)\n",
        "#     Where:\n",
        "#     C = the most recent closing price\n",
        "#     L14 = the low of the 14 previous trading sessions\n",
        "#     H14 = the highest price traded during the same 14-day period\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = close\n",
        "    temp_df['High'] = high\n",
        "    temp_df['Low'] = low\n",
        "    \n",
        "    temp_df['Sto_K'] = ((temp_df.Close - temp_df.Low.rolling(period).min()) / \\\n",
        "                        (temp_df.High.rolling(period).max() - temp_df.Low.rolling(period).min()))\n",
        "    \n",
        "    return np.array(temp_df.Sto_K.values) * 100\n",
        "\n",
        "def Stochastic_D(sto_k,period=3):\n",
        "#     %K = 100(C - L14)/(H14 - L14)\n",
        "#     Where:\n",
        "#     C = the most recent closing price\n",
        "#     L14 = the low of the 14 previous trading sessions\n",
        "#     H14 = the highest price traded during the same 14-day period\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Sto_k'] = sto_k\n",
        "    temp_df['Sto_d'] = temp_df.Sto_k.rolling(period, center = False).mean()\n",
        "    \n",
        "    return np.array(temp_df.Sto_d.values)\n",
        "\n",
        "##  RSI function\n",
        "def RSI(data, period):\n",
        "    # data is a np array, period is size of window\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = data\n",
        "    temp_df['Change'] = temp_df.Close - temp_df.Close.shift(1)\n",
        "    temp_df['Up'] = [i if i>=0 else np.nan for i in temp_df.Change]\n",
        "    temp_df['Down'] = [-i if i<0 else np.nan for i in temp_df.Change]\n",
        "    temp_df['Ave_gain'] = temp_df.Up.rolling(period, min_periods = 1).sum()/period\n",
        "    temp_df['Ave_loss'] = temp_df.Down.rolling(period, min_periods = 1).sum()/period\n",
        "    temp_df['RS'] = temp_df.Ave_gain / temp_df.Ave_loss\n",
        "    temp_df['RSI'] = (100 - (100 / (1 + temp_df['RS']))) \n",
        "    temp = np.array(temp_df.RSI.values)\n",
        "    temp[:period-1] = np.nan\n",
        "    \n",
        "    return temp\n",
        "\n",
        "def Williams_R(close,high,low,period=14):\n",
        "# %R = (highest high – closing price) / (highest high – lowest low) x -100\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = close\n",
        "    temp_df['High'] = high\n",
        "    temp_df['Low'] = low\n",
        "    \n",
        "    temp_df['Wil_R'] = ((temp_df.High.rolling(period).max() - temp_df.Close) / \\\n",
        "                        (temp_df.High.rolling(period).max() - temp_df.Low.rolling(period).min()))\n",
        "    \n",
        "    return np.array(temp_df.Wil_R.values) * -100\n",
        "\n",
        "def MACD(close):\n",
        "# The MACD is calculated by subtracting the 26-day exponential moving average (EMA) \n",
        "# from the 12-day EMA. A nine-day EMA of the MACD, called the \"signal line\", is then \n",
        "# plotted on top of the MACD, functioning as a trigger for buy and sell signals\n",
        "\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = close\n",
        "    MACD = np.array(temp_df.Close.ewm(span = 12).mean() - temp_df.Close.ewm(span = 26).mean())\n",
        "    temp_df2 = pd.DataFrame()\n",
        "    temp_df2['MACD'] = MACD\n",
        "    Signal = np.array(temp_df2.MACD.ewm(span = 9).mean())\n",
        "\n",
        "    return MACD - Signal\n",
        "\n",
        "def Accum_Ditrib(close,high,low,volume):\n",
        "# 1. Money Flow Multiplier = [(Close  -  Low) - (High - Close)] /(High - Low) \n",
        "# 2. Money Flow Volume = Money Flow Multiplier x Volume for the Period\n",
        "# 3. ADL = Previous ADL + Current Period's Money Flow Volume\n",
        "# https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:accumulation_distribution_line\n",
        "\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = close\n",
        "    temp_df['High'] = high\n",
        "    temp_df['Low'] = low\n",
        "    temp_df['Volume'] = volume\n",
        "    \n",
        "    temp_df['MFM'] = ((temp_df.Close - temp_df.Low) - (temp_df.High - temp_df.Close)) / (temp_df.High - temp_df.Low)\n",
        "    temp_df['MFV'] = temp_df.MFM * temp_df.Volume\n",
        "    temp_df['ADL'] = np.cumsum(temp_df.MFV.values)\n",
        "    \n",
        "\n",
        "    return np.array(temp_df.ADL.values)\n",
        "\n",
        "\n",
        "def CCI(close,high,low):\n",
        "    \n",
        "# CCI = (Typical Price  -  20-period SMA of TP) / (.015 x Mean Deviation)\n",
        "# Typical Price (TP) = (High + Low + Close)/3\n",
        "# Constant = .015\n",
        "# There are four steps to calculating the Mean Deviation: \n",
        "# First, subtract the most recent 20-period average of the typical price from each period's typical price. \n",
        "# Second, take the absolute values of these numbers. \n",
        "# Third, sum the absolute values. \n",
        "# Fourth, divide by the total number of periods (20). \n",
        "# https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:commodity_channel_index_cci\n",
        "\n",
        "# The definition of overbought or oversold varies for the Commodity Channel Index (CCI). \n",
        "# ±100 may work in a trading range, but more extreme levels are needed for other situations. \n",
        "# ±200 is a much harder level to reach and more representative of a true extreme\n",
        "\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = close\n",
        "    temp_df['High'] = high\n",
        "    temp_df['Low'] = low\n",
        "    temp_df['TP'] = temp_df['High'] + temp_df['Low'] + temp_df['Close']\n",
        "    temp_df['SMA20'] = temp_df.TP.rolling(20,center = False).mean()\n",
        "    \n",
        "    temp_df['Mean_Dev'] = np.zeros(len(temp_df))\n",
        "    for i in np.arange(0,20):\n",
        "        temp_df['Mean_Dev'] += np.abs(temp_df.TP.shift(i)-temp_df.SMA20)\n",
        "    temp_df.Mean_Dev = temp_df.Mean_Dev /20\n",
        "    \n",
        "    temp_df['CCI'] = (temp_df.TP - temp_df.SMA20) / (0.015 * temp_df.Mean_Dev)\n",
        "\n",
        "    return np.array(temp_df.CCI.values)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rBp8d9vyfGgT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "## Feature engineering\n",
        "### Acknowledgment: Dr. Matloob Khushi \n",
        "#######################################\n",
        "\n",
        "## define trend deterministic functions from technical indicators\n",
        "\n",
        "def T_SMA(sma, close):\n",
        "    return np.where(close>sma,1,np.where(close<sma,-1,0))\n",
        "\n",
        "def T_WMA(wma, close):\n",
        "    return np.where(close>wma,1,np.where(close<wma,-1,0))\n",
        "\n",
        "    # return +1 if sto_k at time t > sto_k at time t-1, and visa versa (0 if neither)\n",
        "def T_Sto_K(sto_k):\n",
        "    # insert 0 in front instead of np.nan\n",
        "    return np.insert(np.where(sto_k[1:]>sto_k[:-1], 1, np.where(sto_k[1:]<sto_k[:-1], -1, 0)),0,0)\n",
        "\n",
        "def T_Sto_D(sto_d):\n",
        "    # return +1 if sto_d at time t > sto_d at time t-1, and visa versa (0 if neither)\n",
        "    # insert 0 in front instead of np.nan\n",
        "    return np.insert(np.where(sto_d[1:]>sto_d[:-1], 1, np.where(sto_d[1:]<sto_d[:-1], -1, 0)),0,0)\n",
        "\n",
        "def T_Williams_R(data):\n",
        "    # return +1 if sto_d at time t > sto_d at time t-1, and visa versa (0 if neither)\n",
        "    # insert 0 in front instead of np.nan\n",
        "    return np.insert(np.where(data[1:]>data[:-1], 1, np.where(data[1:]<data[:-1], -1, 0)),0,0)\n",
        "\n",
        "def T_MACD(data):\n",
        "    # return +1 if data at time t > data at time t-1, and visa versa (0 if neither)\n",
        "    # insert 0 in front instead of np.nan\n",
        "    return np.insert(np.where(data[1:]>data[:-1], 1, np.where(data[1:]<data[:-1], -1, 0)),0,0)\n",
        "\n",
        "def T_RSI(data):\n",
        "    # If the value of RSI exceeds 70 level, it means that the stock\n",
        "    # is overbought, so, it may go down in near future (indicating opinion\n",
        "    # ‘-1’) and if the value of RSI goes below 30 level, it means that\n",
        "    # the stock is oversold, so, it may go up in near future (indicating\n",
        "    # opinion ‘+1’). For the values between (30, 70), if RSI at time ‘t’ is\n",
        "    # greater than RSI at time ‘t-1’, the opinion on trend is represented t\n",
        "    # as ‘+1’ and vice-a-versa\n",
        "    \n",
        "    return np.where(data>70,-1,np.where(data<30,1,\\\n",
        "                                       np.insert(np.where(data[1:]>data[:-1], 1, np.where(data[1:]<data[:-1], -1, 0)),0,0)\\\n",
        "                                       ))\n",
        "\n",
        "\n",
        "def T_CCI(data):\n",
        "    # CCI is also used for identifying overbought and oversold levels\n",
        "    # This means that if CCI value exceeds 200 level, the opinion for\n",
        "    # the trend is ‘-1’ and if it is below -200 level then the opinion\n",
        "    # for the trend is ‘+1’. For the values between (-200, 200), if CCI at\n",
        "    # time ‘t’ is greater than CCI at time ‘t-1’, the opinion on the trend\n",
        "    # is ‘+1’ and vice-a-versa\n",
        "    \n",
        "    return np.where(data>200,-1,np.where(data<-200,1,\\\n",
        "                                       np.insert(np.where(data[1:]>data[:-1], 1, np.where(data[1:]<data[:-1], -1, 0)),0,0)\\\n",
        "                                       ))\n",
        "\n",
        "def T_AD(data):\n",
        "    # A/D oscillator also follows the stock trend meaning that if its value at time ‘t’ \n",
        "    # is greater than that at time ‘t-1’, the opinion on trend is ‘+1’ and vice-a-versa\n",
        "    # insert 0 in front instead of np.nan\n",
        "    \n",
        "    return np.insert(np.where(data[1:]>data[:-1], 1, np.where(data[1:]<data[:-1], -1, 0)),0,0)\n",
        "\n",
        "def T_Momentum(data):\n",
        "    # Momentum measures the rate of rise and fall of stock prices. \n",
        "    # Positive value of momentum indicates up trend and is represented by ‘+1’ \n",
        "    # while negative value indicates down trend and is represented as ‘-1'\n",
        "\n",
        "    return np.where(data>0,1,np.where(data<0,-1,0))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esPHN9tKfGgX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "## Feature engineering\n",
        "### Acknowledgment: Dr. Matloob Khushi \n",
        "#######################################\n",
        "\n",
        "# define functions to calculate second set of technocal indicators\n",
        "\n",
        "def OBV(close, volume):\n",
        "    theta = np.where(close[1:] > close[:-1],1,np.where(close[1:] < close[:-1],-1,0))\n",
        "    theta = np.insert(theta,0,0)\n",
        "    \n",
        "    temp = []\n",
        "    \n",
        "    for i in  np.arange(0,len(close)):\n",
        "        if i == 0:\n",
        "            temp.append(0)\n",
        "        else:\n",
        "            temp.append(temp[-1] + (theta[i] * volume[i]))\n",
        "    \n",
        "    return temp\n",
        "\n",
        "def Bias(close, period):\n",
        "    \n",
        "    ma = SMA(close,period)\n",
        "    \n",
        "    return 100 * (close - ma) / ma\n",
        "\n",
        "def ASY(close, periods):\n",
        "    \n",
        "    SY = np.insert(100 * (np.log(close[1:]) - np.log(close[:-1])),0,0)\n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['SY'] = np.insert(100 * (np.log(close[1:]) - np.log(close[:-1])),0,0)\n",
        "    \n",
        "    return temp_df.SY.rolling(periods).mean()\n",
        "\n",
        "def PSY(close,period):\n",
        "    # number of rsising days in period\n",
        "    theta = np.where(close[1:] > close[:-1],1,np.where(close[1:] < close[:-1],-1,0))\n",
        "    theta = np.insert(theta,0,0)\n",
        "    \n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['rising'] = theta\n",
        "    \n",
        "    return temp_df.rising.rolling(period).sum().values / period\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zgZQAZ6fGgZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#######################################\n",
        "## Feature engineering\n",
        "### Acknowledgment: Dr. Matloob Khushi \n",
        "#######################################\n",
        "\n",
        "# functions to determin gain or loss in future\n",
        "\n",
        "def Move(data, periods):\n",
        "    \n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Close'] = data\n",
        "\n",
        "    return np.array(temp_df.Close.shift(-periods) / temp_df.Close)\n",
        "\n",
        "def T_Move(data):\n",
        "    # return 1 if Move (data) > 0, -1 if <0, otherwise 0\n",
        "    return np.where(data>1,1,np.where(data<1,-1,0))\n",
        "    \n",
        "def High(high, close, periods):\n",
        "    # calculat the ratio of 1) the highest price over next period days, divided by 2) current close\n",
        "    \n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['High'] = high\n",
        "    temp_df['Close'] = close\n",
        "\n",
        "    return np.array(pd.rolling_max(temp_df.High.shift(-1)[::-1], window=periods, min_periods=0)[::-1]/ temp_df.Close)\n",
        "\n",
        "def Low(low, close, periods):\n",
        "    # calculate the ratio of 1) the lowest price over next period days, divided by 2) current close\n",
        "    \n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['Low'] = low\n",
        "    temp_df['Close'] = close\n",
        "\n",
        "    return np.array(pd.rolling_min(temp_df.Low.shift(-1)[::-1], window=periods, min_periods=0)[::-1]/ temp_df.Close)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eql65zusPuMl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "######################################\n",
        "def exponential_smoothing(data_df):\n",
        "  \n",
        "  ## Exponentially weighted Moving Average \n",
        "\n",
        "  ## Reference:\n",
        "  ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.ewm.html, accessed 31 August\n",
        "\n",
        "  alpha = [0.25,0.5,0.75] ## weights\n",
        "\n",
        "  for k in alpha:\n",
        "    data_df[\"EWMA-\"+str(k)] = data_df.Close.ewm(com = k).mean()\n",
        "    if np.sum(np.asarray(data_df[\"EWMA-\"+str(k)].isna() == True)) > 0:\n",
        "      data_df[\"EWMA-\"+str(k)] = data_df[\"EWMA-\"+str(k)].fillna(method = \"bfill\") ## Imputation\n",
        "    else:\n",
        "      continue\n",
        "  return data_df\n",
        "\n",
        "def volatility(data_df): \n",
        "  ## Volatility\n",
        "\n",
        "  ## References\n",
        "  ## https://chrisalbon.com/python/data_wrangling/pandas_moving_average/, accessed 31 August\n",
        "  ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isna.html, accessed 31 August\n",
        "  ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna, accessed 31 August\n",
        "\n",
        "  periods_moving_average = [100,150,200,250]\n",
        "  for k in periods_moving_average:\n",
        "    data_df[\"Volatility-\"+str(k)] = data_df.Close.rolling(window = k).std()\n",
        "    if np.sum(np.asarray(data_df[\"Volatility-\"+str(k)].isna() == True)) > 0:\n",
        "      data_df[\"Volatility-\"+str(k)] = data_df[\"Volatility-\"+str(k)].fillna(method = \"bfill\") ## Imputation\n",
        "    else:\n",
        "      continue\n",
        "  return data_df\n",
        "    \n",
        "    \n",
        "  \n",
        "def autocorrelation(data_df):\n",
        "  ## Autocorrelation \n",
        "\n",
        "  ## References\n",
        "  ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.autocorr.html, 1 September\n",
        "  autocorr_array = []\n",
        "  threshold_lag = 250\n",
        "  row,col = np.shape(data_df)\n",
        "  k = 0\n",
        "  while k < row:\n",
        "    if k >= threshold_lag:\n",
        "      autocorr_array.append(0)\n",
        "    else:\n",
        "      autocorr_array.append(data_df.Close.autocorr(lag = k))\n",
        "    k = k+1\n",
        "\n",
        "  data_df[\"autocorrelation\"] = autocorr_array\n",
        "  return data_df\n",
        "\n",
        "def volume_traded_MA(data_df):\n",
        "\n",
        "  ## References\n",
        "  ## https://chrisalbon.com/python/data_wrangling/pandas_moving_average/, accessed 31 August\n",
        "  ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isna.html, accessed 31 August\n",
        "  ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html#pandas.DataFrame.fillna, accessed 31 August\n",
        "\n",
        "  periods_moving_average = [100,150,200,250]\n",
        "  for k in periods_moving_average:\n",
        "    data_df[\"Volume_MA-\"+str(k)] = data_df.Volume.rolling(window = k).mean()\n",
        "    if np.sum(np.asarray(data_df[\"Volume_MA-\"+str(k)].isna() == True)) > 0:\n",
        "      data_df[\"Volume_MA-\"+str(k)] = data_df[\"Volume_MA-\"+str(k)].fillna(method = \"bfill\") ## Imputation\n",
        "    else:\n",
        "      continue\n",
        "  return data_df  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0rQjeyQTAuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Technical analysis"
      ]
    },
    {
      "metadata": {
        "id": "fAp0snQ1FkBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ce4047b6-3e66-48a3-f1cb-62a008dcdb49"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## TECHNICAL ANALYSIS:\n",
        "\n",
        "## VERSION 2.0 looks at last 3 days of momentum\n",
        "## Version 2.1 looks at last 1 day of momentum which is a calculation of last 5 days of momentum (As defined in the Matloob function)\n",
        "\n",
        "def candlestick_pattern(data,pattern):\n",
        "  \n",
        "  if pattern == \"Hammer v2.0\":\n",
        "\n",
        "    ## Hammer v2.0:\n",
        "\n",
        "    pattern_list = []\n",
        "    alpha = 0.98\n",
        "    beta = 0.98\n",
        "    for i in range(3,data.shape[0]):\n",
        "        count = 0 \n",
        "        ## Checking Momentum\n",
        "        for j in range(3):\n",
        "            if data[\"T_Momentum\"][i-1-j] == -1:\n",
        "                count = count + 1\n",
        "        ## Must be on a downward trend (last 3 days downward). \n",
        "        if count == 3:\n",
        "            ## stock must trade signficantly lower than its opening during the day\n",
        "            if data[\"Low\"][i] < alpha*data[\"Open\"][i]:\n",
        "                ## stock must rally to close significantly above its opening\n",
        "                if data[\"Close\"][i]*beta > data[\"Open\"][i]:\n",
        "                    ## The tail should be at least twice the size of the candlestick body\n",
        "                    if data[\"Open\"][i] - data['Low'][i] >= 2*(data['Close'][i]-data['Open'][i]):\n",
        "                        pattern_list.append(i)\n",
        "    \n",
        "    return pattern_list\n",
        "  \n",
        "  elif pattern == \"Shooting Star v2.0\":\n",
        "  \n",
        "    ## Shooting Star v2.0:\n",
        "\n",
        "    pattern_list = []\n",
        "    alpha = 1.03\n",
        "    for i in range(3,data.shape[0]):\n",
        "        count = 0 \n",
        "        ## Checking Momentum\n",
        "        for j in range(3):\n",
        "            if data[\"T_Momentum\"][i-1-j] == 1:\n",
        "                count = count + 1\n",
        "        ## Must be on an uptrend (last 3 days)   \n",
        "        if count ==3:\n",
        "            ## Stock must have a high significantly greater than its opening \n",
        "            if data[\"High\"][i] > alpha*data[\"Open\"][i]:\n",
        "                ## Despite this stock falls and closes lower than its opening\n",
        "                if data[\"Close\"][i] < data[\"Open\"][i]:\n",
        "                    ## distance between high and open must be more than twice as the body\n",
        "                    if data[\"High\"][i] - data[\"Open\"][i] > 2*np.absolute(data[\"Open\"][i] - data[\"Close\"][i]):\n",
        "                        pattern_list.append(i)\n",
        "\n",
        "    return pattern_list\n",
        "  \n",
        "  \n",
        "  elif pattern == \"Three line strike v2.0\":\n",
        "    ## Three line strike v2.0: \n",
        "\n",
        "    pattern_list = []\n",
        "    for i in range(6,data.shape[0]):\n",
        "        count = 0\n",
        "        ## Checking Momentum\n",
        "        for j in range(3,6):\n",
        "            if data[\"T_Momentum\"][i-1-j] == -1:\n",
        "                count = count + 1\n",
        "        ## Must be in a donward trend (last 3 days)\n",
        "        if count == 3:\n",
        "            ## Must have 4 bars all having lower lows than the next and 4th opening even lower\n",
        "            if data[\"Low\"][i-3] > data[\"Low\"][i-2] > data[\"Low\"][i-1] > data[\"Open\"][i]:\n",
        "                ## Final bar must close for greater than the high of the first bar in the series\n",
        "                if data[\"Close\"][i] > data[\"High\"][i-3]:\n",
        "                    pattern_list.append(i)\n",
        "    \n",
        "    return pattern_list\n",
        "  \n",
        "  elif pattern == \"Morning Star v2.0\":\n",
        "    ## Morning Star v2.0:\n",
        "\n",
        "    alpha = 1.03\n",
        "    beta = 1.03\n",
        "    pattern_list = []\n",
        "    for i in range(5,data.shape[0]):\n",
        "        count = 0 \n",
        "        for j in range(2,5):\n",
        "            if data[\"T_Momentum\"][i-1-j] == -1:\n",
        "                count = count + 1\n",
        "        ## Must be in a downtrend\n",
        "        if count == 3:\n",
        "            ## First bar must close significantly lower than the opening\n",
        "            if data[\"Close\"][i-2]*alpha < data[\"Open\"][i-2]:\n",
        "                ## 2nd bar must open for less than first bar and close for less than first bar:\n",
        "                if data[\"Open\"][i-1] < data[\"Open\"][i-2] and data[\"Close\"][i-1] < data[\"Close\"][i-2]:\n",
        "                    ## Final bar shows a strong resergence and closes for significantly more then it opens for \n",
        "                    if data[\"Close\"][i] > data[\"Open\"][i]*beta:\n",
        "                        pattern_list.append(i)\n",
        "    \n",
        "    return pattern_list\n",
        "  \n",
        "  elif pattern == \"Bullish Abandoned baby v2.1\":\n",
        "  ## Bullish Abandoned baby v2.1: \n",
        "\n",
        "    pattern_list = []\n",
        "    alpha = 1.01\n",
        "    beta = 1.01\n",
        "    for i in range(3, data.shape[0]):\n",
        "        ## Check the day before the pattern to see if we are on a downward trend\n",
        "        if data[\"T_Momentum\"][i-3] == -1:\n",
        "            ## First bar must be bearish\n",
        "            if data[\"Close\"][i-2] < data[\"Open\"][i-2]:\n",
        "                ## Second bar closes for lower than the lowest point of the first bar\n",
        "                if data[\"Close\"][i-1] < data[\"Low\"][i-2]:\n",
        "                    ## Second bar must also be a doji candle (Very small body):\n",
        "                    if np.absolute(data[\"Close\"][i-1]-data[\"Open\"][i-1]) < 0.01*data[\"Close\"][i-1]:\n",
        "                        ## Third bar must open for significantly greater than the 2nd bar close.\n",
        "                        if data[\"Close\"][i-1]*alpha < data[\"Open\"][i]:\n",
        "                            ## Finally first bar must close for significantly more than it opened for\n",
        "                            if data[\"Close\"][i] > data[\"Open\"][i]*beta:\n",
        "                                pattern_list.append(i)\n",
        "  \n",
        "    return pattern_list\n",
        "  \n",
        "  \n",
        "  elif pattern == \"Morning Star v2.1\":\n",
        "    ## Morning Star v2.1:\n",
        "\n",
        "    alpha = 1.03\n",
        "    beta = 1.05\n",
        "    pattern_list = []\n",
        "    for i in range(3,data.shape[0]):\n",
        "        ## Check the day before the pattern to see if we are on a downward trend\n",
        "        if data[\"T_Momentum\"][i-3] == -1:\n",
        "            ## First bar must close significantly lower than the opening\n",
        "            if data[\"Close\"][i-2]*alpha < data[\"Open\"][i-2]:\n",
        "                ## 2nd bar must gap such that its body does not touch body of prior bar:\n",
        "                if np.max([data[\"Close\"][i-1],data[\"Open\"][i-1]]) < data[\"Close\"][i-2]:\n",
        "                    ## Final bar shows a strong resergence and closes for significantly more then it opens for \n",
        "                    if data[\"Close\"][i] > data[\"Open\"][i]*beta:\n",
        "                        pattern_list.append(i)\n",
        "\n",
        "    return pattern_list\n",
        "  \n",
        "  elif pattern == \"Three line strike v2.1\":\n",
        "  ## Three line strike v2.1: \n",
        "\n",
        "    pattern_list = []\n",
        "    for i in range(4,data.shape[0]):\n",
        "        ## Check the day before the pattern to see if we are on a downward trend\n",
        "        if data[\"T_Momentum\"][i-4] == -1:\n",
        "            ## Must have 4 bars all having lower lows than the next and 4th opening even lower\n",
        "            if data[\"Low\"][i-3] > data[\"Low\"][i-2] > data[\"Low\"][i-1] > data[\"Open\"][i]:\n",
        "                ## Final bar must close for greater than the high of the first bar in the series\n",
        "                if data[\"Close\"][i] > data[\"High\"][i-3]:\n",
        "                    pattern_list.append(i)\n",
        "    \n",
        "    return pattern_list\n",
        "  \n",
        "  elif pattern == \"Hammer v2.1\":\n",
        "    ## Hammer v2.1:\n",
        "\n",
        "    pattern_list = []\n",
        "    alpha = 0.98\n",
        "    beta = 0.98\n",
        "    for i in range(3,data.shape[0]):\n",
        "        ## Check the day before the pattern to see if we are on a downward trend\n",
        "        if data[\"T_Momentum\"][i-1] == -1:\n",
        "            ## stock must trade signficantly lower than its opening during the day\n",
        "            if data[\"Low\"][i] < alpha*data[\"Open\"][i]:\n",
        "                ## stock must rally to close significantly above its opening\n",
        "                if data[\"Close\"][i]*beta > data[\"Open\"][i]:\n",
        "                    ## The tail should be at least twice the size of the candlestick body\n",
        "                    if data[\"Open\"][i] - data['Low'][i] >= 2*(data['Close'][i]-data['Open'][i]):\n",
        "                        pattern_list.append(i)\n",
        "    return pattern_list\n",
        "  \n",
        "  elif pattern == \"Shooting Star v2.1\":\n",
        "    ## Shooting Star v2.1:\n",
        "\n",
        "    pattern_list = []\n",
        "    alpha = 1.03\n",
        "    for i in range(3,data.shape[0]):\n",
        "         ## Check the day before the pattern to see if we are on an uptrend\n",
        "        if data[\"T_Momentum\"][i-1] == 1:\n",
        "            ## Stock must have a high significantly greater than its opening \n",
        "            if data[\"High\"][i] > alpha*data[\"Open\"][i]:\n",
        "                ## Despite this stock falls and closes lower than its opening\n",
        "                if data[\"Close\"][i] < data[\"Open\"][i]:\n",
        "                    ## distance between high and open must be more than twice as the body\n",
        "                    if data[\"High\"][i] - data[\"Open\"][i] > 2*np.absolute(data[\"Open\"][i] - data[\"Close\"][i]):\n",
        "                        pattern_list.append(i)\n",
        "    return pattern_list\n",
        "  \n",
        "  else:\n",
        "    print(\"Wrong type of candlestick pattern\")\n",
        "    \n",
        "\n",
        "\n",
        "def results_observer(data,pattern):\n",
        "  pattern_list = candlestick_pattern(data,pattern)\n",
        "  ## Results observer v2.0 (Requires running one of the candlestick pattern code blocks first: \n",
        "  closes = []\n",
        "  new_prices = []\n",
        "  for index in pattern_list:\n",
        "      closes.append(data[\"Close\"][index])\n",
        "      new_prices.append(data[\"Close\"][index+1])\n",
        "\n",
        "  increase_count = 0 \n",
        "  zero_count = 0\n",
        "  decrease_count = 0\n",
        "  for i in range(len(new_prices)):\n",
        "      if new_prices[i] == closes[i]:\n",
        "          zero_count = zero_count + 1\n",
        "      elif new_prices[i] > closes[i]:\n",
        "          increase_count = increase_count + 1\n",
        "      elif new_prices[i] < closes[i]:\n",
        "          decrease_count = decrease_count + 1 \n",
        "\n",
        "  print(\"Increase likelihood is: \", increase_count/(len(new_prices)))\n",
        "  print(\"No movement liklihood is: \", zero_count/(len(new_prices)))\n",
        "  print(\"Decrease liklihood is: \", decrease_count/(len(new_prices)))\n",
        "\n",
        "  return {\"Increase probability\":increase_count,\"Probability of no movement\":zero_count,\"Decrease probability\":decrease_count}\n",
        "  \n",
        "  \n",
        "## To convert a pattern list into a column:\n",
        "\n",
        "def make_column(data,pattern,title):\n",
        "  pattern_list = candlestick_pattern(data,pattern)\n",
        "  column = []\n",
        "  for i in range(data.shape[0]):\n",
        "    if i in pattern_list:\n",
        "      column.append(1)\n",
        "    else:\n",
        "      column.append(0)\n",
        "  data[title] = column\n",
        "  \n",
        "## Example\n",
        "    \n",
        "## make_column(pattern_list, \"shooting star\")\n",
        "\"\"\"\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## TECHNICAL ANALYSIS:\\n\\n## VERSION 2.0 looks at last 3 days of momentum\\n## Version 2.1 looks at last 1 day of momentum which is a calculation of last 5 days of momentum (As defined in the Matloob function)\\n\\ndef candlestick_pattern(data,pattern):\\n  \\n  if pattern == \"Hammer v2.0\":\\n\\n    ## Hammer v2.0:\\n\\n    pattern_list = []\\n    alpha = 0.98\\n    beta = 0.98\\n    for i in range(3,data.shape[0]):\\n        count = 0 \\n        ## Checking Momentum\\n        for j in range(3):\\n            if data[\"T_Momentum\"][i-1-j] == -1:\\n                count = count + 1\\n        ## Must be on a downward trend (last 3 days downward). \\n        if count == 3:\\n            ## stock must trade signficantly lower than its opening during the day\\n            if data[\"Low\"][i] < alpha*data[\"Open\"][i]:\\n                ## stock must rally to close significantly above its opening\\n                if data[\"Close\"][i]*beta > data[\"Open\"][i]:\\n                    ## The tail should be at least twice the size of the candlestick body\\n                    if data[\"Open\"][i] - data[\\'Low\\'][i] >= 2*(data[\\'Close\\'][i]-data[\\'Open\\'][i]):\\n                        pattern_list.append(i)\\n    \\n    return pattern_list\\n  \\n  elif pattern == \"Shooting Star v2.0\":\\n  \\n    ## Shooting Star v2.0:\\n\\n    pattern_list = []\\n    alpha = 1.03\\n    for i in range(3,data.shape[0]):\\n        count = 0 \\n        ## Checking Momentum\\n        for j in range(3):\\n            if data[\"T_Momentum\"][i-1-j] == 1:\\n                count = count + 1\\n        ## Must be on an uptrend (last 3 days)   \\n        if count ==3:\\n            ## Stock must have a high significantly greater than its opening \\n            if data[\"High\"][i] > alpha*data[\"Open\"][i]:\\n                ## Despite this stock falls and closes lower than its opening\\n                if data[\"Close\"][i] < data[\"Open\"][i]:\\n                    ## distance between high and open must be more than twice as the body\\n                    if data[\"High\"][i] - data[\"Open\"][i] > 2*np.absolute(data[\"Open\"][i] - data[\"Close\"][i]):\\n                        pattern_list.append(i)\\n\\n    return pattern_list\\n  \\n  \\n  elif pattern == \"Three line strike v2.0\":\\n    ## Three line strike v2.0: \\n\\n    pattern_list = []\\n    for i in range(6,data.shape[0]):\\n        count = 0\\n        ## Checking Momentum\\n        for j in range(3,6):\\n            if data[\"T_Momentum\"][i-1-j] == -1:\\n                count = count + 1\\n        ## Must be in a donward trend (last 3 days)\\n        if count == 3:\\n            ## Must have 4 bars all having lower lows than the next and 4th opening even lower\\n            if data[\"Low\"][i-3] > data[\"Low\"][i-2] > data[\"Low\"][i-1] > data[\"Open\"][i]:\\n                ## Final bar must close for greater than the high of the first bar in the series\\n                if data[\"Close\"][i] > data[\"High\"][i-3]:\\n                    pattern_list.append(i)\\n    \\n    return pattern_list\\n  \\n  elif pattern == \"Morning Star v2.0\":\\n    ## Morning Star v2.0:\\n\\n    alpha = 1.03\\n    beta = 1.03\\n    pattern_list = []\\n    for i in range(5,data.shape[0]):\\n        count = 0 \\n        for j in range(2,5):\\n            if data[\"T_Momentum\"][i-1-j] == -1:\\n                count = count + 1\\n        ## Must be in a downtrend\\n        if count == 3:\\n            ## First bar must close significantly lower than the opening\\n            if data[\"Close\"][i-2]*alpha < data[\"Open\"][i-2]:\\n                ## 2nd bar must open for less than first bar and close for less than first bar:\\n                if data[\"Open\"][i-1] < data[\"Open\"][i-2] and data[\"Close\"][i-1] < data[\"Close\"][i-2]:\\n                    ## Final bar shows a strong resergence and closes for significantly more then it opens for \\n                    if data[\"Close\"][i] > data[\"Open\"][i]*beta:\\n                        pattern_list.append(i)\\n    \\n    return pattern_list\\n  \\n  elif pattern == \"Bullish Abandoned baby v2.1\":\\n  ## Bullish Abandoned baby v2.1: \\n\\n    pattern_list = []\\n    alpha = 1.01\\n    beta = 1.01\\n    for i in range(3, data.shape[0]):\\n        ## Check the day before the pattern to see if we are on a downward trend\\n        if data[\"T_Momentum\"][i-3] == -1:\\n            ## First bar must be bearish\\n            if data[\"Close\"][i-2] < data[\"Open\"][i-2]:\\n                ## Second bar closes for lower than the lowest point of the first bar\\n                if data[\"Close\"][i-1] < data[\"Low\"][i-2]:\\n                    ## Second bar must also be a doji candle (Very small body):\\n                    if np.absolute(data[\"Close\"][i-1]-data[\"Open\"][i-1]) < 0.01*data[\"Close\"][i-1]:\\n                        ## Third bar must open for significantly greater than the 2nd bar close.\\n                        if data[\"Close\"][i-1]*alpha < data[\"Open\"][i]:\\n                            ## Finally first bar must close for significantly more than it opened for\\n                            if data[\"Close\"][i] > data[\"Open\"][i]*beta:\\n                                pattern_list.append(i)\\n  \\n    return pattern_list\\n  \\n  \\n  elif pattern == \"Morning Star v2.1\":\\n    ## Morning Star v2.1:\\n\\n    alpha = 1.03\\n    beta = 1.05\\n    pattern_list = []\\n    for i in range(3,data.shape[0]):\\n        ## Check the day before the pattern to see if we are on a downward trend\\n        if data[\"T_Momentum\"][i-3] == -1:\\n            ## First bar must close significantly lower than the opening\\n            if data[\"Close\"][i-2]*alpha < data[\"Open\"][i-2]:\\n                ## 2nd bar must gap such that its body does not touch body of prior bar:\\n                if np.max([data[\"Close\"][i-1],data[\"Open\"][i-1]]) < data[\"Close\"][i-2]:\\n                    ## Final bar shows a strong resergence and closes for significantly more then it opens for \\n                    if data[\"Close\"][i] > data[\"Open\"][i]*beta:\\n                        pattern_list.append(i)\\n\\n    return pattern_list\\n  \\n  elif pattern == \"Three line strike v2.1\":\\n  ## Three line strike v2.1: \\n\\n    pattern_list = []\\n    for i in range(4,data.shape[0]):\\n        ## Check the day before the pattern to see if we are on a downward trend\\n        if data[\"T_Momentum\"][i-4] == -1:\\n            ## Must have 4 bars all having lower lows than the next and 4th opening even lower\\n            if data[\"Low\"][i-3] > data[\"Low\"][i-2] > data[\"Low\"][i-1] > data[\"Open\"][i]:\\n                ## Final bar must close for greater than the high of the first bar in the series\\n                if data[\"Close\"][i] > data[\"High\"][i-3]:\\n                    pattern_list.append(i)\\n    \\n    return pattern_list\\n  \\n  elif pattern == \"Hammer v2.1\":\\n    ## Hammer v2.1:\\n\\n    pattern_list = []\\n    alpha = 0.98\\n    beta = 0.98\\n    for i in range(3,data.shape[0]):\\n        ## Check the day before the pattern to see if we are on a downward trend\\n        if data[\"T_Momentum\"][i-1] == -1:\\n            ## stock must trade signficantly lower than its opening during the day\\n            if data[\"Low\"][i] < alpha*data[\"Open\"][i]:\\n                ## stock must rally to close significantly above its opening\\n                if data[\"Close\"][i]*beta > data[\"Open\"][i]:\\n                    ## The tail should be at least twice the size of the candlestick body\\n                    if data[\"Open\"][i] - data[\\'Low\\'][i] >= 2*(data[\\'Close\\'][i]-data[\\'Open\\'][i]):\\n                        pattern_list.append(i)\\n    return pattern_list\\n  \\n  elif pattern == \"Shooting Star v2.1\":\\n    ## Shooting Star v2.1:\\n\\n    pattern_list = []\\n    alpha = 1.03\\n    for i in range(3,data.shape[0]):\\n         ## Check the day before the pattern to see if we are on an uptrend\\n        if data[\"T_Momentum\"][i-1] == 1:\\n            ## Stock must have a high significantly greater than its opening \\n            if data[\"High\"][i] > alpha*data[\"Open\"][i]:\\n                ## Despite this stock falls and closes lower than its opening\\n                if data[\"Close\"][i] < data[\"Open\"][i]:\\n                    ## distance between high and open must be more than twice as the body\\n                    if data[\"High\"][i] - data[\"Open\"][i] > 2*np.absolute(data[\"Open\"][i] - data[\"Close\"][i]):\\n                        pattern_list.append(i)\\n    return pattern_list\\n  \\n  else:\\n    print(\"Wrong type of candlestick pattern\")\\n    \\n\\n\\ndef results_observer(data,pattern):\\n  pattern_list = candlestick_pattern(data,pattern)\\n  ## Results observer v2.0 (Requires running one of the candlestick pattern code blocks first: \\n  closes = []\\n  new_prices = []\\n  for index in pattern_list:\\n      closes.append(data[\"Close\"][index])\\n      new_prices.append(data[\"Close\"][index+1])\\n\\n  increase_count = 0 \\n  zero_count = 0\\n  decrease_count = 0\\n  for i in range(len(new_prices)):\\n      if new_prices[i] == closes[i]:\\n          zero_count = zero_count + 1\\n      elif new_prices[i] > closes[i]:\\n          increase_count = increase_count + 1\\n      elif new_prices[i] < closes[i]:\\n          decrease_count = decrease_count + 1 \\n\\n  print(\"Increase likelihood is: \", increase_count/(len(new_prices)))\\n  print(\"No movement liklihood is: \", zero_count/(len(new_prices)))\\n  print(\"Decrease liklihood is: \", decrease_count/(len(new_prices)))\\n\\n  return {\"Increase probability\":increase_count,\"Probability of no movement\":zero_count,\"Decrease probability\":decrease_count}\\n  \\n  \\n## To convert a pattern list into a column:\\n\\ndef make_column(data,pattern,title):\\n  pattern_list = candlestick_pattern(data,pattern)\\n  column = []\\n  for i in range(data.shape[0]):\\n    if i in pattern_list:\\n      column.append(1)\\n    else:\\n      column.append(0)\\n  data[title] = column\\n  \\n## Example\\n    \\n## make_column(pattern_list, \"shooting star\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "jEfw45JOTEYw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### feature_engineer function"
      ]
    },
    {
      "metadata": {
        "id": "Tqgu5-KlGQf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## A function that performs feature engineering \n",
        "## Acknowledgment: Dr. Matloob Khushi \n",
        "\n",
        "\n",
        "def feature_engineer(data_original):\n",
        "  symbol_asx = data_original.symbol\n",
        "  adjusted_data_df = data_original\n",
        "    \n",
        "  ## add columns to adjusted_data_df dataframe\n",
        "  # Name and technical indicators\n",
        "\n",
        "  adjusted_data_df['SMA'] = SMA(adjusted_data_df.Close.values,10)\n",
        "  adjusted_data_df['WMA'] = WMA(adjusted_data_df.Close.values,10)\n",
        "  adjusted_data_df['Momentum'] = Momentum(adjusted_data_df.Close.values,10)\n",
        "  adjusted_data_df['RSI'] = RSI(adjusted_data_df.Close.values,10)\n",
        "  adjusted_data_df['Sto_K'] = Stochastic_K(adjusted_data_df.Close.values,adjusted_data_df.High.values,adjusted_data_df.Low.values,14)\n",
        "  adjusted_data_df['Sto_D'] = Stochastic_D(adjusted_data_df.Sto_K.values,3)\n",
        "  adjusted_data_df['Wil_R'] = Williams_R(adjusted_data_df.Close.values,adjusted_data_df.High.values,adjusted_data_df.Low.values,14)\n",
        "  adjusted_data_df['MACD'] = MACD(adjusted_data_df.Close.values) # MACD - Signal\n",
        "  adjusted_data_df['AD'] = Accum_Ditrib(adjusted_data_df.Close.values,adjusted_data_df.High.values,adjusted_data_df.Low.values,adjusted_data_df.Volume.values)\n",
        "  adjusted_data_df['CCI'] = CCI(adjusted_data_df.Close.values,adjusted_data_df.High.values,adjusted_data_df.Low.values)\n",
        "  # Trend deterministic technical indicators\n",
        "\n",
        "  ## Could take into consideration later\n",
        "  adjusted_data_df['T_SMA'] = T_SMA(adjusted_data_df.SMA.values,adjusted_data_df.Close.values)\n",
        "  adjusted_data_df['T_WMA'] = T_WMA(adjusted_data_df.WMA.values,adjusted_data_df.Close.values)\n",
        "  adjusted_data_df['T_Sto_K'] = T_Sto_K(adjusted_data_df.Sto_K.values)\n",
        "  adjusted_data_df['T_Sto_D'] = T_Sto_D(adjusted_data_df.Sto_D.values)\n",
        "  adjusted_data_df['T_Wil_R'] = T_Williams_R(adjusted_data_df.Wil_R.values)\n",
        "  adjusted_data_df['T_MACD'] = T_MACD(adjusted_data_df.MACD.values)\n",
        "  adjusted_data_df['T_RSI'] = T_RSI(adjusted_data_df.RSI.values)\n",
        "  adjusted_data_df['T_CCI'] = T_CCI(adjusted_data_df.CCI.values)\n",
        "  adjusted_data_df['T_AD'] = T_AD(adjusted_data_df.AD.values)\n",
        "  adjusted_data_df['T_Momentum'] = T_Momentum(adjusted_data_df.Momentum.values)\n",
        "\n",
        "\n",
        "  # Set 2 Technical Indicators\n",
        "  adjusted_data_df['OBV'] = OBV(adjusted_data_df.Close.values,adjusted_data_df.Volume.values)\n",
        "  adjusted_data_df['SMA_5'] = SMA(adjusted_data_df.Close.values,5)\n",
        "  adjusted_data_df['Bias'] = Bias(adjusted_data_df.Close.values,6)\n",
        "  adjusted_data_df['PSY'] = PSY(adjusted_data_df.Close.values,12)\n",
        "  for i in np.arange(1,6): ## Removed for loops below for speed\n",
        "    adjusted_data_df['ASY'+str(i)] = ASY(adjusted_data_df.Close.values,i)\n",
        "\n",
        "    # Gain/Loss x days ahead\n",
        "    #for i in np.arange(1,11):\n",
        "    temp = \"Move\"+str(i)\n",
        "    adjusted_data_df[temp] = Move(adjusted_data_df.Close.values,i)\n",
        "\n",
        "    #for i in np.arange(1,11):\n",
        "    temp = \"T_Move\"+str(i)\n",
        "    temp1 = \"Move\"+str(i)\n",
        "    adjusted_data_df[temp] = T_Move(adjusted_data_df[temp1].values)\n",
        "\n",
        "    #for i in np.arange(1,11):\n",
        "    temp = \"High\"+str(i)\n",
        "    adjusted_data_df[temp] = High(adjusted_data_df.High.values,adjusted_data_df.Close.values,i)\n",
        "\n",
        "    #for i in np.arange(1,11):\n",
        "    temp = \"Low\"+str(i)\n",
        "    adjusted_data_df[temp] = Low(adjusted_data_df.Low.values,adjusted_data_df.Close.values,i)\n",
        "\n",
        "\n",
        "  exponential_smoothing(adjusted_data_df)\n",
        "  volatility(adjusted_data_df)\n",
        "  autocorrelation(adjusted_data_df)\n",
        "  volume_traded_MA(adjusted_data_df)\n",
        "  \n",
        "  \"\"\"\n",
        "  pattern_array = [\"Hammer v2.0\",\"Shooting Star v2.0\",\"Three line strike v2.0\",\"Morning Star v2.0\",\n",
        "                \"Bullish Abandoned baby v2.1\",\"Morning Star v2.1\",\"Three line strike v2.1\",\n",
        "                \"Hammer v2.1\",\"Shooting Star v2.1\"]\n",
        "  \n",
        "  for pattern in pattern_array:\n",
        "    adjusted_data_df[pattern] = make_column(adjusted_data_df,pattern,title = pattern)\n",
        "  \"\"\"\n",
        "  \n",
        "  return adjusted_data_df\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WGi6mycUTOaN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set indices for later use (training dataset)"
      ]
    },
    {
      "metadata": {
        "id": "_f9--2_NbuZw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9b18472b-c1d9-4eca-fe2b-5166508f750b"
      },
      "cell_type": "code",
      "source": [
        "## Setting indices for later use #############\n",
        "\n",
        "## Set seed for random shuffling\n",
        "np.random.seed(1)\n",
        "\n",
        "## To use with dataset_list\n",
        "## Using random integers\n",
        "## https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random_integers.html#numpy.random.random_integers, accessed 6 October\n",
        "max_index_dataset_list = 4\n",
        "index_array_dataset_list = np.random.random_integers(low = 1, high = max_index_dataset_list)\n",
        "print(index_array_dataset_list)\n",
        "\n",
        "## To use with name_list\n",
        "## Set threshold at 200 arbitrarily\n",
        "train_size = 20 ## Training data set size\n",
        "\n",
        "## Sorting array:\n",
        "## https://www.programiz.com/python-programming/methods/built-in/sorted\n",
        "## accessed 7 October\n",
        "index_array_name_list = np.unique(sorted(np.random.random_integers(low = 1,size = train_size,high = 150),reverse = False))\n",
        "print(len(index_array_name_list))\n",
        "\n",
        "## Size of dataset to left out for testing\n",
        "test_size = 10\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: This function is deprecated. Please call randint(1, 4 + 1) instead\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: This function is deprecated. Please call randint(1, 150 + 1) instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "84fuacgyTZNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Perform feature engineering using the *feature_engineer* function"
      ]
    },
    {
      "metadata": {
        "id": "Fsaozt58ZDKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "fbcb12d8-8ec5-4199-a73f-402c850c4be4"
      },
      "cell_type": "code",
      "source": [
        "## Removing duplicates:\n",
        "## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html accessed 7 September\n",
        "\n",
        "adjusted_data_df_subset = {}\n",
        "adjusted_data_df_merge = {}\n",
        "adj_data_df_subset = {}\n",
        "\n",
        "## Recall:\n",
        "dataset_list = [\"ASX\",\"SP500_1\",\"SP500_2\",\"SP500_3\"]\n",
        "\n",
        "dataset_list = [dataset_list[0],dataset_list[index_array_dataset_list]]\n",
        "print(dataset_list)\n",
        "\n",
        "for data in dataset_list: ## Adjust this so that runtime is reduced \n",
        "  \n",
        "  ## Adjust this so that runtime is reduced \n",
        "  name_list = adjusted_data_df_original[data].symbol.drop_duplicates()[index_array_name_list]\n",
        "  \n",
        "  \n",
        "  row,col = np.shape(adjusted_data_df_original[data])\n",
        "  adjusted_data_df_subset[data] = {}\n",
        "  adjusted_data_df_merge[data] = {}\n",
        "  adj_data_df_subset[data] = {}\n",
        "\n",
        "  for name in name_list:\n",
        "    \n",
        "    \n",
        "    adjusted_data_df_subset[data][name] = adjusted_data_df_original[data].loc[adjusted_data_df_original[data].symbol == name,:]\n",
        "    adjusted_data_df_merge[data][name] = merge_wbdata(adjusted_data_df_subset[data][name],name)\n",
        "    \n",
        "    adj_data_df_subset[data][name] = feature_engineer(adjusted_data_df_merge[data][name])\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ASX', 'SP500_2']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in less\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in greater\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in less\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in greater\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in less\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in greater\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in less\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in less\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: RuntimeWarning: invalid value encountered in less\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: RuntimeWarning: invalid value encountered in less\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in less\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: RuntimeWarning: invalid value encountered in less\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: pd.rolling_max is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=1,min_periods=0,center=False).max()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: pd.rolling_min is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=1,min_periods=0,center=False).min()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: pd.rolling_max is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=2,min_periods=0,center=False).max()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: pd.rolling_min is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=2,min_periods=0,center=False).min()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: pd.rolling_max is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=3,min_periods=0,center=False).max()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: pd.rolling_min is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=3,min_periods=0,center=False).min()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: pd.rolling_max is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=4,min_periods=0,center=False).max()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: pd.rolling_min is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=4,min_periods=0,center=False).min()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: FutureWarning: pd.rolling_max is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=5,min_periods=0,center=False).max()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: FutureWarning: pd.rolling_min is deprecated for Series and will be removed in a future version, replace with \n",
            "\tSeries.rolling(window=5,min_periods=0,center=False).min()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in subtract\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in subtract\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "BaIzwTjITibX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Normalize"
      ]
    },
    {
      "metadata": {
        "id": "frwcQojMzlkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "  ## Standard deviation in Numpy\n",
        "  ## https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html, accessed 17 September\n",
        "  return (data - np.mean(data))/np.std(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y2xzjYOU3riR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Perform normalization\n",
        "\n",
        "## Drop columns:\n",
        "## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\n",
        "## accessed 21 September\n",
        "\n",
        "def perform_normalize():\n",
        "  adj_data_df_work = {}\n",
        "  for data in dataset_list[:max_index_dataset_list]:\n",
        "    adj_data_df_work[data] = {}\n",
        "    name_list = adjusted_data_df_original[data].symbol.drop_duplicates()[index_array_name_list]\n",
        "    for name in name_list:\n",
        "      adj_data_df_work[data][name] = adj_data_df_subset[data][name].dropna(axis = \"columns\",how = \"any\")\n",
        "      adj_data_df_work[data][name] = adj_data_df_work[data][name].drop(columns = \"symbol\")\n",
        "      for cc in adj_data_df_work[data][name].columns:\n",
        "        adj_data_df_work[data][name][cc] = normalize(adj_data_df_work[data][name][cc])\n",
        "\n",
        "  return adj_data_df_work\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Abyf8ePnrQH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "adj_data_df_work = perform_normalize()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9_r5Gmi-KmR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## A list of stock names for reference\n",
        "\n",
        "def print_keys():\n",
        "  name_store = {}\n",
        "  for data in dataset_list[:max_index_dataset_list]:\n",
        "    name_store[data] = {}\n",
        "    name_list = adjusted_data_df_original[data].symbol.drop_duplicates()[index_array_name_list]\n",
        "    for name in name_list:\n",
        "      name_store[data][name] = [data,name,np.shape(adj_data_df_work[data][name])]\n",
        "  return name_store\n",
        "\n",
        "store_name = print_keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MbzQLQ-YSyqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "b56ca3ae-8658-451f-bf48-945049de076e"
      },
      "cell_type": "code",
      "source": [
        "store_name"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ASX': {'ASX:AST': ['ASX', 'ASX:AST', (3208, 51)],\n",
              "  'ASX:BAP': ['ASX', 'ASX:BAP', (1100, 51)],\n",
              "  'ASX:CPU': ['ASX', 'ASX:CPU', (4753, 50)],\n",
              "  'ASX:FMG': ['ASX', 'ASX:FMG', (4753, 50)],\n",
              "  'ASX:FPH': ['ASX', 'ASX:FPH', (4262, 50)],\n",
              "  'ASX:GNC': ['ASX', 'ASX:GNC', (4753, 50)],\n",
              "  'ASX:GUD': ['ASX', 'ASX:GUD', (4753, 50)],\n",
              "  'ASX:ILU': ['ASX', 'ASX:ILU', (4753, 50)],\n",
              "  'ASX:IRE': ['ASX', 'ASX:IRE', (4536, 50)],\n",
              "  'ASX:LLC': ['ASX', 'ASX:LLC', (4753, 50)],\n",
              "  'ASX:ORI': ['ASX', 'ASX:ORI', (4753, 50)],\n",
              "  'ASX:PGH': ['ASX', 'ASX:PGH', (1186, 51)],\n",
              "  'ASX:PLS': ['ASX', 'ASX:PLS', (2764, 51)],\n",
              "  'ASX:PRY': ['ASX', 'ASX:PRY', (4753, 50)],\n",
              "  'ASX:QAN': ['ASX', 'ASX:QAN', (4753, 50)],\n",
              "  'ASX:QBE': ['ASX', 'ASX:QBE', (4753, 50)],\n",
              "  'ASX:QUB': ['ASX', 'ASX:QUB', (2944, 51)],\n",
              "  'ASX:REA': ['ASX', 'ASX:REA', (4753, 50)],\n",
              "  'ASX:RIO': ['ASX', 'ASX:RIO', (4753, 50)],\n",
              "  'ASX:RRL': ['ASX', 'ASX:RRL', (4752, 50)]},\n",
              " 'SP500_2': {'HBAN': ['SP500_2', 'HBAN', (5974, 37)],\n",
              "  'HES': ['SP500_2', 'HES', (5974, 37)],\n",
              "  'INTU': ['SP500_2', 'INTU', (5974, 37)],\n",
              "  'JWN': ['SP500_2', 'JWN', (5974, 37)],\n",
              "  'KHC': ['SP500_2', 'KHC', (812, 52)],\n",
              "  'KIM': ['SP500_2', 'KIM', (5971, 37)],\n",
              "  'KR': ['SP500_2', 'KR', (5974, 37)],\n",
              "  'LH': ['SP500_2', 'LH', (5974, 37)],\n",
              "  'LYB': ['SP500_2', 'LYB', (2117, 52)],\n",
              "  'MAR': ['SP500_2', 'MAR', (5974, 37)],\n",
              "  'MU': ['SP500_2', 'MU', (5974, 37)],\n",
              "  'NDAQ': ['SP500_2', 'NDAQ', (4087, 50)],\n",
              "  'NEE': ['SP500_2', 'NEE', (5974, 37)],\n",
              "  'NFX': ['SP500_2', 'NFX', (5974, 37)],\n",
              "  'NKE': ['SP500_2', 'NKE', (5974, 37)],\n",
              "  'NKTR': ['SP500_2', 'NKTR', (5974, 37)],\n",
              "  'NLSN': ['SP500_2', 'NLSN', (1927, 52)],\n",
              "  'NOC': ['SP500_2', 'NOC', (5974, 37)],\n",
              "  'NRG': ['SP500_2', 'NRG', (3728, 50)],\n",
              "  'NTAP': ['SP500_2', 'NTAP', (5749, 37)]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "jHDxssVqgef2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exploratory analysis\n",
        "\n",
        "We will create plots in order to explore the attributes as part of the process of data cleaning. Since the data size is extremely large, our exploratory analysis will need to restrict to a certain subset."
      ]
    },
    {
      "metadata": {
        "id": "Z9169doZCh_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "163444fa-31c5-4256-aea1-b1bdfe367ebb"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Testing only for AAPL data\n",
        "aapl = adj_data_df_work[\"SP500_1\"][\"AAPL\"]\n",
        "aapl.columns\n",
        "\"\"\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Testing only for AAPL data\\naapl = adj_data_df_work[\"SP500_1\"][\"AAPL\"]\\naapl.columns\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "-OuCqomGq6HV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Assessing correlation structures - Heatmap\n",
        "\n",
        "This is to explore correlation structures and to look for clusters. If there are clusters, we could then perform some dimensionality reduction on those clusters."
      ]
    },
    {
      "metadata": {
        "id": "s1bsodKe0JqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Function to create heatmap. This will be used to explore correlation structures\n",
        "\n",
        "def heatmap_plot(data_name,stock_abb,chosen_attr):\n",
        "  ## For \"stock_abb\": Put \"ASX:\" then put the desired stock abbreviation\n",
        "  ## adj_data_df_work is a dictionary with key = stock name, values = dataset for that stock\n",
        "  ## Chosen attribute: Numerical column index chosen\n",
        "  \n",
        "  adj_data_df = adj_data_df_work[data_name][stock_abb]\n",
        "  adj_data_df = adj_data_df.iloc[:,chosen_attr]\n",
        "  corr_mat = np.matrix(adj_data_df.corr())\n",
        "  ## Seaborn heatmap:\n",
        "  ## https://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap, accessed September 9\n",
        "  plt.figure(figsize = (15,15))\n",
        "  sns.heatmap(adj_data_df.corr(),annot = True)\n",
        "  plt.title(\"Heatmap plot for \"+str(stock_abb))\n",
        "  plt.show()\n",
        "\n",
        "  ## Computation of eigenvalues:\n",
        "  ## https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eigvals.html, accessed 16 September\n",
        "  ## https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html?highlight=correlation%20matrix, accessed 17 September\n",
        "def eigen_stuff(data_name,stock_abb,chosen_attr):\n",
        "  adj_data_df = adj_data_df_work[data_name][stock_abb]\n",
        "  adj_data_df = adj_data_df.iloc[:,chosen_attr]\n",
        "  evals,evec = np.linalg.eig(np.corrcoef(adj_data_df))\n",
        "  \n",
        "  ## Taking real part:\n",
        "  ## https://docs.scipy.org/doc/numpy/reference/generated/numpy.real.html, accessed 18 September\n",
        "  \n",
        "  return evals, evec\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9XbqCv08csDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db2f93ab-1283-46c7-d5e7-5feefd529d9e"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "aapl = adj_data_df_work[\"SP500_1\"][\"AAPL\"]\n",
        "heatmap_plot(\"SP500_1\",\"AAPL\",range(10,24))\n",
        "\"\"\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\naapl = adj_data_df_work[\"SP500_1\"][\"AAPL\"]\\nheatmap_plot(\"SP500_1\",\"AAPL\",range(10,24))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "RBDtGoYsIcmi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Time series plot"
      ]
    },
    {
      "metadata": {
        "id": "iqSp9usRwueH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "97889b9f-7c26-42d2-82b6-0d33075b830e"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Plotting\n",
        "## Idea of time series plot:\n",
        "## http://pandas.pydata.org/pandas-docs/version/0.13/visualization.html, accessed 2 October\n",
        "for data in dataset_list:\n",
        "  print(data)  \n",
        "  for name in name_list[:4]:\n",
        "    plt.figure(figsize = (15,8))\n",
        "    print(adj_data_df_work[data][name].Close)\n",
        "    sns.tsplot(adj_data_df_work[data][name].Close)\n",
        "    plt.title(name)\n",
        "    plt.show()\n",
        "    \n",
        "\"\"\" "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Plotting\\n## Idea of time series plot:\\n## http://pandas.pydata.org/pandas-docs/version/0.13/visualization.html, accessed 2 October\\nfor data in dataset_list:\\n  print(data)  \\n  for name in name_list[:4]:\\n    plt.figure(figsize = (15,8))\\n    print(adj_data_df_work[data][name].Close)\\n    sns.tsplot(adj_data_df_work[data][name].Close)\\n    plt.title(name)\\n    plt.show()\\n    \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "ASOgb7VHr8_k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data analysis using State-space model\n",
        "\n",
        "Sources that could be used:\n",
        "\n",
        "- https://docs.scipy.org/doc/numpy-1.10.4/reference/generated/numpy.matrix.tolist.html (accessed 5 September)\n",
        "- https://pydlm.github.io/pydlm_user_guide.html#modeling, accessed 5 September\n",
        "- https://pydlm.github.io/example1.html#dynamic-linear-regression, accessed 5 September\n",
        "- https://github.com/wwrechard/pydlm/blob/master/doc/source/pydlm_user_guide.rst, accessed 11 September\n"
      ]
    },
    {
      "metadata": {
        "id": "JTCzxuDBD64P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split the data into training and test sets.\n",
        "\n",
        "We will train the State-space model on all but the last five observations. The estimated mean will be used as the prediction, and the performance will be assessed against the left-out observations, which corresponds to the last five observations."
      ]
    },
    {
      "metadata": {
        "id": "rL93efGvbSBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c758de35-ef87-4a60-f4b4-37a902b35173"
      },
      "cell_type": "code",
      "source": [
        "## Optional #############\n",
        "\n",
        "\"\"\"\n",
        "## Split data into training and test set\n",
        "train_data = {}\n",
        "test_data = {}\n",
        "for name in name_list:\n",
        "  row,_ = np.shape(adj_data_df_work[name])\n",
        "  train_data[name] = adj_data_df_work[name].iloc[np.arange(0,row-5),:]\n",
        "  test_data[name] = adj_data_df_work[name].iloc[np.arange(row-5,row),:]\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## Split data into training and test set\\ntrain_data = {}\\ntest_data = {}\\nfor name in name_list:\\n  row,_ = np.shape(adj_data_df_work[name])\\n  train_data[name] = adj_data_df_work[name].iloc[np.arange(0,row-5),:]\\n  test_data[name] = adj_data_df_work[name].iloc[np.arange(row-5,row),:]\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "1wmKxEKM2E1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#test_data[\"ASX:ABP\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "maYVd2uiEbvj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare data for processing with _pydlm_ package\n",
        "\n",
        "The pydlm package requires some manipukation on the structures of the data. For example, list of lists will be needed for the training response to be fed into the dlm package in order to perform dynamic linear model."
      ]
    },
    {
      "metadata": {
        "id": "CmrEq1b6G3Cj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e271bd5-183f-47c9-bd8e-fca5900edb6d"
      },
      "cell_type": "code",
      "source": [
        "response_train = {}\n",
        "feature_matrix_dd_train = {}\n",
        "\n",
        "## AD did not appear in ASX, so in order to have the same number of columns,\n",
        "## AD must also be removed for SP500_1\n",
        "column_drop = [\"Close\",\"adjusted_close\",\"Open\",\"High\",\"Low\"]\n",
        "\n",
        "\n",
        "## Train on S&P500, test on ASX ########################\n",
        "## Create training set for S&P500\n",
        "\n",
        "## data_train: SP500_1\n",
        "\n",
        "finished = 0\n",
        "\n",
        "while finished == 0: ## While the execution is not finished\n",
        "  data_train = dataset_list[1]\n",
        "  data = data_train\n",
        "  \n",
        "  print(\"Training data:\",data)\n",
        "  response_train[data] = {}\n",
        "  feature_matrix_dd_train[data] = {}\n",
        "  \n",
        "  ## name_list for training set\n",
        "  ## Use the first 20 indices\n",
        "  \n",
        "  name_list_train = list(adjusted_data_df_original[data].symbol.drop_duplicates()[index_array_name_list[:train_size]])\n",
        "  \n",
        "  for name_tr in name_list_train: \n",
        "    response_train[data][name_tr] = list(adj_data_df_work[data][name_tr].loc[:,\"Close\"])\n",
        "\n",
        "    ## Convert pandas object to list:\n",
        "    ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.tolist.html (accessed 21 September)\n",
        "\n",
        "    ## Drop a column\n",
        "    ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html (accessed 21 September)\n",
        "\n",
        "    ## Convert data type:\n",
        "    ## https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html#numpy.ndarray.astype\n",
        "    ## accessed 21 September\n",
        "    response_train[data][name_tr] = [[r] for r in adj_data_df_work[data][name_tr].loc[:,\"Close\"]]\n",
        "    feature_matrix_dd_train[data][name_tr] = np.matrix(adj_data_df_work[data][name_tr].drop(columns = column_drop)).astype(dtype = \"float\").tolist()\n",
        "  \n",
        "  finished = finished + 1\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data: SP500_2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tDgeDHEjT780",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Set indices (test set)"
      ]
    },
    {
      "metadata": {
        "id": "rnUOUXzYdL4S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "## Size of dataset to left out for testing\n",
        "print(test_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZevqSRtLQJ_4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create data matrix for the test set (to be fed into prediction in _pydlm_)"
      ]
    },
    {
      "metadata": {
        "id": "Mm5bJt0NadaC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Create test set for ASX\n",
        "\n",
        "## data_test: ASX\n",
        "\n",
        "response_test = {}\n",
        "feature_matrix_dd_test = {}\n",
        "column_drop = [\"Close\",\"adjusted_close\",\"Open\",\"High\",\"Low\"]\n",
        "\n",
        "\n",
        "finished = 0 \n",
        "\n",
        "while finished == 0: ## While the execution is not finished\n",
        "  data_test = dataset_list[0]\n",
        "  data = data_test\n",
        "  response_test[data] = {}\n",
        "  feature_matrix_dd_test[data] = {}\n",
        "  \n",
        "  length_name_list_test = len(list(adjusted_data_df_original[data_test].symbol.drop_duplicates()))\n",
        "  name_list_test = list(adjusted_data_df_original[data].symbol.drop_duplicates()[index_array_name_list[:test_size]])\n",
        "  \n",
        "  for name_te in name_list_test:\n",
        "    num_row = adj_data_df_work[data][name_te].shape[0]\n",
        "    response_test[data][name_te] = adj_data_df_work[data][name_te].Close.iloc[index_array_name_list[:test_size]]\n",
        "\n",
        "    ## Convert pandas object to list:\n",
        "    ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.tolist.html (accessed 21 September)\n",
        "\n",
        "    ## Drop a column\n",
        "    ## https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html (accessed 21 September)\n",
        "\n",
        "    ## Convert data type:\n",
        "    ## https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html#numpy.ndarray.astype\n",
        "    ## accessed 21 September\n",
        "    \n",
        "    ##\n",
        "    feature_matrix_dd_test[data][name_te] = np.matrix(adj_data_df_work[data][name_te].iloc[index_array_name_list[:test_size]].\n",
        "                                                      drop(columns = column_drop)).astype(dtype = \"float\").tolist()\n",
        "  finished = finished + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tUzBzL0-wzCk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2129a79e-813b-4208-94be-8b4082c2e24d"
      },
      "cell_type": "code",
      "source": [
        "print(\"Train:\",name_list_train)\n",
        "print(\"Test:\",name_list_test)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: ['HBAN', 'HES', 'INTU', 'JWN', 'KHC', 'KIM', 'KR', 'LH', 'LYB', 'MAR', 'MU', 'NDAQ', 'NEE', 'NFX', 'NKE', 'NKTR', 'NLSN', 'NOC', 'NRG', 'NTAP']\n",
            "Test: ['ASX:AST', 'ASX:BAP', 'ASX:CPU', 'ASX:FPH', 'ASX:FMG', 'ASX:GUD', 'ASX:GNC', 'ASX:ILU', 'ASX:IRE', 'ASX:LLC']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vhrL2oA6E07d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Implement the dynamic linear model to obtain prediction mean\n",
        "\n",
        "The implementation will yield the predicted mean and others such as predicted variance for the five test observations. However, for some stocks, the predicted mean will be missing. Thus, we will take on the method of imputing the mean value of the training response values for that stock. This is done in the impute_missing_pred_mean function."
      ]
    },
    {
      "metadata": {
        "id": "XRI2NogAq8yR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## We will work with data from adj_data_df_work\n",
        "## data_train: SP500_1\n",
        "## Train dynamic linear model on SP500 ###############\n",
        "\n",
        "## This stores the dlm object, and this will be indexed by stock names in training data\n",
        "mydlm = {}\n",
        "\n",
        "\n",
        "name_list_train = list(adjusted_data_df_original[data_train].symbol.drop_duplicates()[index_array_name_list[:train_size]])\n",
        "name_list_test = list(adjusted_data_df_original[data_test].symbol.drop_duplicates()[index_array_name_list[:test_size]])\n",
        "\n",
        "for name_tr in name_list_train: \n",
        "  print(\"=========\",name_tr,\"============\")\n",
        "\n",
        "  mydlm[name_tr] = {}\n",
        "  \n",
        "  mydlm[name_tr] = dlm(response_train[data_train][name_tr]) + dynamic(features = \n",
        "                                                               feature_matrix_dd_train[data_train][name_tr],\n",
        "                                                               discount = 1,name = name_tr)\n",
        "  mydlm[name_tr].fitForwardFilter()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9QmwLJhtWRln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A look at the latent states ($\\beta$ coefficients)"
      ]
    },
    {
      "metadata": {
        "id": "KvJNaBPEtJ0y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Get coefficients for the state-space model (DLM) ###############\n",
        "\n",
        "\n",
        "data_train = dataset_list[1]\n",
        "data_test = dataset_list[0]\n",
        "\n",
        "latent_coef_mean = {}\n",
        "latent_coef_var = {}\n",
        "our_state = {}\n",
        "\n",
        "for name_tr in name_list_train: \n",
        "  print(\"=========\",name_tr,\"============\")\n",
        "  latent_coef_mean[name_tr] = mydlm[name_tr].getLatentState(filterType = \"forwardFilter\")\n",
        "  latent_coef_var[name_tr] = mydlm[name_tr].getLatentCov(filterType = \"forwardFilter\")\n",
        " \n",
        "  our_state[name_tr] = latent_coef_mean[name_tr][len(latent_coef_mean[name_tr])-1]\n",
        "  if np.sum(np.isnan(latent_coef_mean[name_tr]) == True) > 0:\n",
        "    our_state[name_tr] = [np.mean(response_train[data_train][name_tr])]*len(our_state[name_tr])\n",
        "  else:\n",
        "    our_state[name_tr] = latent_coef_mean[name_tr][len(latent_coef_mean[name_tr])-1]\n",
        " \n",
        "  our_state[name_tr] = np.reshape(our_state[name_tr],newshape = (len(our_state[name_tr]),1))\n",
        "  print(our_state[name_tr])\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ozBwa7gB0uJJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## To predict ASX testing data from SP500_1 training data. If missing values are\n",
        "## present, impute with average training response (SP500_1 in this case)\n",
        "\n",
        "def predict_test_train(name_list_test,name_list_train):\n",
        "  \n",
        "  pred_test_train = {}\n",
        "  \n",
        "  for name_te in name_list_test:\n",
        "    pred_test_train[name_te] = {} \n",
        "    for name_tr in name_list_train:\n",
        "      if np.shape(our_state[name_tr])[0] == np.shape(feature_matrix_dd_test[data_test][name_te])[1]:\n",
        "        pred_test_train[name_te][name_tr] = list(np.dot(feature_matrix_dd_test[data_test][name_te],our_state[name_tr]).flatten())\n",
        "        if np.sum(np.isnan(pred_test_train[name_te][name_tr]) == True) > 0:\n",
        "          pred_test_train[name_te][name_tr] = response_train[data_train][name_tr][len(response_train[data_train][name_tr]) - 1] + np.nan_to_num(pred_test_train[name_te][name_tr])\n",
        "      else:\n",
        "        pred_test_train[name_te][name_tr] = np.nan_to_num(our_state[name_tr]) + np.mean(response_train[data_train][name_tr])\n",
        "\n",
        "  return pred_test_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pBtWUsSrzUiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Testing only\n",
        "predict_test_train(name_list_test,name_list_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2QHcMqyFs05",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Error calculation and performance analysis\n",
        "\n",
        "We will need to see how the prediction performs against the test observations. The metric we will use are mean-squared error, mean absolute percentage error, and absolute percentage error. Each of these will be carried out twice, one for the prediction on the normalized test data, and another on the original scale.\n",
        "\n",
        "To undertake performance analysis on the original scale, we multiply the predictions with the standard deviation of the original close values (for each stock), and add the mean, again from the original close values. This is an inverse transformation that brings the prediction into the original space from the normalized space."
      ]
    },
    {
      "metadata": {
        "id": "z0OVUXsk9eKc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Compute error and fill in missing values (NaN)\n",
        "## It is assumed that if missing values occur, then it can be replaced\n",
        "## by the mean value\n",
        "\n",
        "def error_MSE(pred,truth):\n",
        "  error_sq = [(p-t)**2 for p,t in zip(pred,truth)]\n",
        "  MSE = np.mean(error_sq)\n",
        "  return MSE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mK9qya09iw2d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## ## Prediction of stocks - On normalized scale\n",
        "\n",
        "## Mean\n",
        "\n",
        "prediction_mean_imp = predict_test_train(name_list_test,name_list_train)\n",
        "\n",
        "MSE_error_pred = {}\n",
        "\n",
        "\n",
        "for name_te in name_list_test: \n",
        "  MSE_error_pred[name_te] = {}\n",
        "  for name_tr in name_list_train:\n",
        "    MSE_error_pred[name_te][name_tr] = error_MSE(prediction_mean_imp[name_te][name_tr],response_test[data_test][name_te])\n",
        "    print(\"MSE for:\",name_tr,\"-\",name_te,\"=\",MSE_error_pred[name_te][name_tr])\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZVPrpCngkRsc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## De-normalize - to get to the original scale\n",
        "\n",
        "## We are testing on the \"ASX space\", and this means we have to adjust our normalized\n",
        "## estimates (de-normalized) our estimates based on the mean and covariance matrix of the\n",
        "## ASX space.\n",
        "\n",
        "def get_parameter(data_test,name_test):\n",
        "  ## Standard deviation in Numpy\n",
        "  ## https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html, accessed 17 September\n",
        "  std = np.std(adj_data_df_subset[data_test][name_test].loc[:,\"Close\"])\n",
        "  mean = np.mean(adj_data_df_subset[data_test][name_test].loc[:,\"Close\"])\n",
        "  \n",
        "  return mean, std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4jdalAR0heFh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Prediction of stocks - Original scale\n",
        "\n",
        "## MSE and RMSE\n",
        "MSE_pred_original_scale = {}\n",
        "RMSE_pred_original_scale = {}\n",
        "pred_original_scale = {}\n",
        "response_test_original_scale = {}\n",
        "\n",
        "\n",
        "## Original data from above (unnormalized): adjusted_data_df_subset[data][name]\n",
        "\n",
        "## response_test[data][name_te]\n",
        "\n",
        "for name_te in name_list_test: \n",
        "  pred_original_scale[name_te] = {}\n",
        "  MSE_pred_original_scale[name_te] = {}\n",
        "  RMSE_pred_original_scale[name_te] = {}\n",
        "  \n",
        "  std_temp, mean_temp = get_parameter(data_test,name_te)\n",
        "  \n",
        "  \n",
        "  ## Response transformed to original scale\n",
        "  response_test_original_scale[name_te] = [std_temp*y + mean_temp for y in response_test[data_test][name_te]]\n",
        "  \n",
        "  for name_tr in name_list_train:    \n",
        "    pred_original_scale[name_te][name_tr] = [std_temp*y + mean_temp for y in prediction_mean_imp[name_te][name_tr]]\n",
        "    \n",
        "    MSE_pred_original_scale[name_te][name_tr] = error_MSE(pred_original_scale[name_te][name_tr],response_test_original_scale[name_te])\n",
        "    RMSE_pred_original_scale[name_te][name_tr] = np.sqrt(MSE_pred_original_scale[name_te][name_tr])\n",
        "    \n",
        "    \n",
        "    print(\"RMSE for:\",name_tr,\"-\",name_te,\"=\",RMSE_pred_original_scale[name_te][name_tr])\n",
        "    print(\"\\n\")\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y4GbJNORPmCd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "performance_data = pd.DataFrame.from_dict(RMSE_pred_original_scale)\n",
        "for q in range(len(performance_data.columns)):\n",
        "  print(\"================\",\"RMSE:\",performance_data.columns[q],\"==================\")\n",
        "  print(\"Mean\",np.mean(performance_data.iloc[:,q]))\n",
        "  print(\"Median\",np.median(performance_data.iloc[:,q]))\n",
        "  print(\"Standard deviation\",np.std(performance_data.iloc[:,q]))\n",
        "  print(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}